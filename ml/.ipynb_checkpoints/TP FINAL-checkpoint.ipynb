{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "supps = [7, 23, 57, 83, 91, 110, 9, 20, 89, 5, 21, 25, 27, 58, 64, 66, 75, 84, 86, 87, 90, 101, 111, 119, 16, 42, \n",
    "         85, 102, 108, 40, 3, 31, 26, 30, 37, 50, 52, 68, 79, 92, 112]\n",
    "\n",
    "cores = [18, 19, 23, 49, 59, 73, 78, 99, 104, 1, 6, 8, 9, 10, 12, 32, 35, 46, 48, 70, 72, 80, 89, 95, 106, 114, \n",
    "         120, 17, 21, 25, 34, 53, 75, 28, 42, 54, 60, 69, 71, 77, 81, 102, 4, 11, 15, 41, 44, 47, 56, 61, 63, 67, \n",
    "         82, 93, 94, 109, 113, 36, 39, 43, 52, 74, 76]\n",
    "\n",
    "idToHero = {1: 'antimage', 2: 'axe', 3: 'bane', 4: 'bloodseeker', 5: 'crystal_maiden', 6: 'drow_ranger', \n",
    "            7: 'earthshaker', 8: 'juggernaut', 9: 'mirana', 11: 'nevermore', 10: 'morphling', 12: 'phantom_lancer', \n",
    "            13: 'puck', 14: 'pudge', 15: 'razor', 16: 'sand_king', 17: 'storm_spirit', 18: 'sven', 19: 'tiny', \n",
    "            20: 'vengefulspirit', 21: 'windrunner', 22: 'zuus', 23: 'kunkka', 25: 'lina', 31: 'lich', 26: 'lion', \n",
    "            27: 'shadow_shaman', 28: 'slardar', 29: 'tidehunter', 30: 'witch_doctor', 32: 'riki', 33: 'enigma', \n",
    "            34: 'tinker', 35: 'sniper', 36: 'necrolyte', 37: 'warlock', 38: 'beastmaster', 39: 'queenofpain', \n",
    "            40: 'venomancer', 41: 'faceless_void', 42: 'skeleton_king', 43: 'death_prophet', 44: 'phantom_assassin',\n",
    "            45: 'pugna', 46: 'templar_assassin', 47: 'viper', 48: 'luna', 49: 'dragon_knight', 50: 'dazzle', 51: \n",
    "            'rattletrap', 52: 'leshrac', 53: 'furion', 54: 'life_stealer', 55: 'dark_seer', 56: 'clinkz', \n",
    "            57: 'omniknight', 58: 'enchantress', 59: 'huskar', 60: 'night_stalker', 61: 'broodmother', \n",
    "            62: 'bounty_hunter', 63: 'weaver', 64: 'jakiro', 65: 'batrider', 66: 'chen', 67: 'spectre', \n",
    "            69: 'doom_bringer', 68: 'ancient_apparition', 70: 'ursa', 71: 'spirit_breaker', 72: 'gyrocopter', \n",
    "            73: 'alchemist', 74: 'invoker', 75: 'silencer', 76: 'obsidian_destroyer', 77: 'lycan', \n",
    "            78: 'brewmaster', 79: 'shadow_demon', 80: 'lone_druid', 81: 'chaos_knight', 82: 'meepo', 83: 'treant', \n",
    "            84: 'ogre_magi', 85: 'undying', 86: 'rubick', 87: 'disruptor', 88: 'nyx_assassin', 89: 'naga_siren', \n",
    "            90: 'keeper_of_the_light', 91: 'wisp', 92: 'visage', 93: 'slark', 94: 'medusa', 95: 'troll_warlord', \n",
    "            96: 'centaur', 97: 'magnataur', 98: 'shredder', 99: 'bristleback', 100: 'tusk', 101: 'skywrath_mage', \n",
    "            102: 'abaddon', 103: 'elder_titan', 104: 'legion_commander', 106: 'ember_spirit', 107: 'earth_spirit', \n",
    "            109: 'terrorblade', 110: 'phoenix', 111: 'oracle', 105: 'techies', 112: 'winter_wyvern', \n",
    "            113: 'arc_warden', 108: 'abyssal_underlord', 114: 'monkey_king', 120: 'pangolier', 119: 'dark_willow'}\n",
    "\n",
    "import time\n",
    "\n",
    "#mirana=9, bane=4 sinergia=0.0315556044752\n",
    "#faceless void=41, phantom assassin=44 sinergia=-0.0205830863765\n",
    "# earthshaker=7, phantom lancer=12\n",
    "\n",
    "\n",
    "\n",
    "print(len(supps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0545901751471\n"
     ]
    }
   ],
   "source": [
    "print(wr_counters[7,12] - wr_counters[12,7] - wr_pairs[7,7] + wr_pairs[12,12])\n",
    "\n",
    "# print(syn_table\n",
    "#       [41,44])\n",
    "\n",
    "# winrates = [(wr_pairs[hero, hero], hero) for hero in cores]\n",
    "# print(winrates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532349679577\n"
     ]
    }
   ],
   "source": [
    "# winrates.sort()\n",
    "# print(winrates)\n",
    "\n",
    "# print(idToHero[winrates[-1][1]], winrates[-6][1])\n",
    "# print(idToHero[winrates[-2][1]], winrates[-2][1])\n",
    "# print(idToHero[winrates[-7][1]], winrates[-7][1])\n",
    "# print(idToHero[winrates[-4][1]], winrates[-4][1])\n",
    "# print(idToHero[winrates[-5][1]], winrates[-5][1])\n",
    "\n",
    "\n",
    "time1 = [1, 60, 83, 85, 43]\n",
    "time2 = [17, 42, 108, 68, 31]\n",
    "# heroes = [71,42,6,81,4]\n",
    "winrate = [wr_pairs[hero, hero] for hero in time2]\n",
    "avg = np.mean(winrate)\n",
    "print(avg)\n",
    "\n",
    "#supps = 110, 37, 83, 20, 85\n",
    "# cores = [71,42,6,81,4]\n",
    "\n",
    "# time bom: Morphling, Lycanthrope, Centaur, Elder Titan e Skywrath Mage\n",
    "# bom: 12, 77, 96, 104, 101 \n",
    "\n",
    "\n",
    "\n",
    "# print([idToHero[id] for id in time1])\n",
    "# print([idToHero[id] for id in time2])\n",
    "\n",
    "#time 1: Anti-Mage, Night Stalker, Treant, Undying e Death Prophet\n",
    "#time 2: Storm Spirit, Wraith King, Underlord, Ancient Apparation e Lich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43095457,  0.56904543]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_input(rad, dire):\n",
    "    input = np.zeros((121 + 45))\n",
    "    for hero in  rad:\n",
    "        input[hero] = 1\n",
    "    for hero in dire:\n",
    "        input[hero] = -1\n",
    "    \n",
    "    counter = 121\n",
    "    \n",
    "    for id1 in range(5):\n",
    "            for id2 in range(id1+1,5):\n",
    "                hero1 = rad[id1]\n",
    "                hero2 = rad[id2]\n",
    "                input[counter] = wr_pairs[hero1, hero2] - (wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]) / 2 \n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "              \n",
    "    #print(\"SYN DIRE\")    \n",
    "    for id1 in range(5):\n",
    "            for id2 in range(id1+1,5):\n",
    "                hero1 = dire[id1]\n",
    "                hero2 = dire[id2]\n",
    "                input[counter] = wr_pairs[hero1, hero2] - (wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]) / 2 \n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "            \n",
    "    #print(\"COUNTER\")    \n",
    "    for hero1 in rad:\n",
    "            for hero2 in dire:\n",
    "                input[counter] = wr_counters[hero1, hero2] - wr_counters[hero2, hero1] - wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]\n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "                \n",
    "    return [input]\n",
    "    \n",
    "time1 = [1, 60, 83, 85, 43]\n",
    "time2 = [17, 42, 108, 68, 31]\n",
    "\n",
    "clf.predict_proba(gen_input(time2, time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_heroes(match):\n",
    "    rad = [id for (id, team) in enumerate(match) if team == 1]\n",
    "    dire = [id for (id, team) in enumerate(match) if team == -1]\n",
    "    print('Rad:', [idToHero[id] for id in rad])\n",
    "    print('Dire:', [idToHero[id] for id in dire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.loadtxt('dota_683.txt', dtype=np.int64)\n",
    "# m = m[ : 50000]\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synnergy(lines):\n",
    "    matches_pairs = np.zeros((121, 121))\n",
    "    wins_pairs = np.zeros((121, 121)) \n",
    "    wr_pairs = np.zeros((121, 121))\n",
    "\n",
    "    for line in lines:\n",
    "        rad_wins = line[1] == 1\n",
    "        dire_wins = not rad_wins\n",
    "        heroes = line[3:]\n",
    "        rad = heroes[:5]\n",
    "        dire = heroes[5:]\n",
    "\n",
    "        for team in [rad, dire]:\n",
    "            for hero1 in team:\n",
    "                for hero2 in team:\n",
    "                    matches_pairs[hero1, hero2] += 1\n",
    "\n",
    "                    if rad_wins and hero1 in rad:\n",
    "                        wins_pairs[hero1, hero2] += 1 \n",
    "                    elif dire_wins and hero2 in dire:\n",
    "                        wins_pairs[hero1, hero2] += 1\n",
    "\n",
    "    matches_pairs[matches_pairs == 0] = 1\n",
    "    wr_pairs = wins_pairs / matches_pairs\n",
    "    return wr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.25169713 -0.27826646 ...,  0.         -0.2437221\n",
      "  -0.21907086]\n",
      " [-0.25169713  0.          0.02730422 ..., -0.25169713  0.02274823\n",
      "  -0.03683651]\n",
      " [-0.27826646  0.02730422  0.         ..., -0.27826646  0.01705699\n",
      "  -0.04680729]\n",
      " ..., \n",
      " [ 0.         -0.25169713 -0.27826646 ...,  0.         -0.2437221\n",
      "  -0.21907086]\n",
      " [-0.2437221   0.02274823  0.01705699 ..., -0.2437221   0.         -0.0697112 ]\n",
      " [-0.21907086 -0.03683651 -0.04680729 ..., -0.21907086 -0.0697112   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def syn():\n",
    "    syn_table = np.zeros((121, 121))\n",
    "    \n",
    "    for hero1 in range(121):\n",
    "        for hero2 in range(121):\n",
    "            syn_table[hero1, hero2] = wr_pairs[hero1, hero2] - (wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]) / 2\n",
    "            \n",
    "    return syn_table\n",
    "\n",
    "syn_table = syn()\n",
    "print(syn_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.125536519432905, ('shadow_demon', 'oracle')), (0.12818858598813154, ('storm_spirit', 'chen')), (0.13036633791310481, ('enchantress', 'brewmaster')), (0.13103294589786985, ('meepo', 'centaur')), (0.13303111038556803, ('lycan', 'medusa')), (0.14324991228822137, ('chen', 'undying')), (0.1526630047103299, ('vengefulspirit', 'chen')), (0.15343109044586578, ('dark_seer', 'centaur')), (0.16442374394511927, ('tidehunter', 'broodmother')), (0.19194608412547898, ('lone_druid', 'oracle'))]\n"
     ]
    }
   ],
   "source": [
    "heroes = [(syn_table[hero1, hero2], (idToHero.get(hero1, \"undefined\"), idToHero.get(hero2, \"undefined\"))) for hero1 in range(121) for hero2 in range(121) if hero1 < hero2]\n",
    "heroes.sort()\n",
    "print(heroes[-10:])\n",
    "\n",
    "\n",
    "# def choose_best(table):\n",
    "#     best_heroes = [(-1,-1)\n",
    "#     val = 0\n",
    "    \n",
    "#     for hero1 in range(121):\n",
    "#         for hero2 in range(121):\n",
    "#             if table[hero1, hero2] > val:\n",
    "#                 val = table[hero1, hero2]\n",
    "#                 best_heroes = (hero1, hero2)\n",
    "            \n",
    "#     return best_heroes\n",
    "\n",
    "# hero1, hero2 = choose_best(syn_table)\n",
    "# print(idToHero[hero1],idToHero[hero2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counters(lines):\n",
    "    matches_counter = np.zeros((121, 121))\n",
    "    wins_counter = np.zeros((121, 121)) \n",
    "    wr_counter = np.zeros((121, 121))\n",
    "\n",
    "    for line in lines:\n",
    "        rad_wins = line[1] == 1\n",
    "        dire_wins = not rad_wins\n",
    "        heroes = line[3:]\n",
    "        rad = heroes[:5]\n",
    "        dire = heroes[5:]\n",
    "\n",
    "        for hero1 in rad:\n",
    "            for hero2 in dire:\n",
    "                matches_counter[hero1, hero2] += 1\n",
    "                matches_counter[hero2, hero1] += 1\n",
    "                if rad_wins: wins_counter[hero1, hero2] += 1\n",
    "                else: wins_counter[hero2, hero1] += 1\n",
    "\n",
    "    wins_counter[matches_counter == 0] = 1\n",
    "    matches_counter[matches_counter == 0] = 1\n",
    "    wr_counter = wins_counter / matches_counter\n",
    "    return wr_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.shuffle(m)\n",
    "factor = 0.9\n",
    "train_size = int(len(m) * factor)\n",
    "wr_pairs = compute_synnergy(m[: train_size])\n",
    "wr_counters = compute_counters(m[ : train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wr_counters[50, 63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(lines):\n",
    "#     inputs = np.zeros((len(lines), 55))\n",
    "    inputs = np.zeros((len(lines), 121+45))\n",
    "    outputs = np.zeros((len(lines)))\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        counter = 0\n",
    "        \n",
    "        line = lines[i]\n",
    "        input = inputs[i]\n",
    "        outputs[i] = line[1]\n",
    "        game_type = line[2]\n",
    "        \n",
    "        heroes = line[3:]\n",
    "        rad = heroes[:5]\n",
    "        dire = heroes[5:]\n",
    "        \n",
    "        #print(\"RAD\")\n",
    "        for hero in rad:\n",
    "            input[hero] = 1\n",
    "#             input[counter] = wr_pairs[hero, hero]\n",
    "            #print(input[counter])\n",
    "#             counter += 1\n",
    "\n",
    "        #print(\"DIRE\")    \n",
    "        for hero in dire:\n",
    "            input[hero] = -1\n",
    "#             input[counter] = wr_pairs[hero, hero]\n",
    "            #print(input[counter])\n",
    "#             counter += 1\n",
    "            \n",
    "        counter = 121    \n",
    "        \n",
    "        #print(\"SYN RAD\")\n",
    "        for id1 in range(5):\n",
    "            for id2 in range(id1+1,5):\n",
    "                hero1 = rad[id1]\n",
    "                hero2 = rad[id2]\n",
    "                input[counter] = wr_pairs[hero1, hero2] - (wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]) / 2 \n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "              \n",
    "        #print(\"SYN DIRE\")    \n",
    "        for id1 in range(5):\n",
    "            for id2 in range(id1+1,5):\n",
    "                hero1 = dire[id1]\n",
    "                hero2 = dire[id2]\n",
    "                input[counter] = wr_pairs[hero1, hero2] - (wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]) / 2 \n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "            \n",
    "        #print(\"COUNTER\")    \n",
    "        for hero1 in rad:\n",
    "            for hero2 in dire:\n",
    "                input[counter] = wr_counters[hero1, hero2] - wr_counters[hero2, hero1] - wr_pairs[hero1, hero1] + wr_pairs[hero2, hero2]\n",
    "                #print(input[counter])\n",
    "                counter += 1\n",
    "                \n",
    "#         counter = 121\n",
    "        \n",
    "#         input[counter] = len([hero for hero in rad if hero in supps])\n",
    "#         counter += 1\n",
    "        \n",
    "#         input[counter] = len([hero for hero in dire if hero in supps])\n",
    "#         counter += 1\n",
    "        \n",
    "#         input[counter] = len([hero for hero in rad if hero in cores])\n",
    "#         counter += 1\n",
    "        \n",
    "#         input[counter] = len([hero for hero in dire if hero in cores])\n",
    "#         counter += 1\n",
    "\n",
    "    return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -1.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      " -1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.          0.\n",
      "  1.          0.          0.          0.          0.         -1.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      " -1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.         -1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.10287483  0.15470215  0.06914453  0.07504428\n",
      "  0.13418471  0.12661056  0.20603503  0.07347422  0.08527426  0.13095135\n",
      "  0.09746259  0.02042123  0.01515661 -0.01751401  0.05945622  0.04361633\n",
      "  0.06431857 -0.01284906 -0.09090597  0.005608    0.00458881 -0.08258415\n",
      " -0.03160051  0.13042525  0.03759838 -0.04868306  0.11234556  0.29331806\n",
      "  0.08727855 -0.00301572 -0.01428613  0.00743129  0.21389421  0.02632751\n",
      "  0.08436239 -0.07892531 -0.22229724  0.14336662 -0.1263452   0.01846141\n",
      "  0.18720025  0.07343729  0.14761367  0.04202461  0.14238718]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs, outputs = format_data(m)\n",
    "#print(len(inputs))\n",
    "print(inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = inputs[10]\n",
    "#match[35] = 0\n",
    "print(len([v for v in match if v <= -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10000,10000]\n\t [[Node: dense_12/kernel/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@dense_12/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dense_12/kernel, dense_12/random_uniform)]]\n\nCaused by op 'dense_12/kernel/Assign', defined at:\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-108-81a427e4334b>\", line 20, in <module>\n    model.add(Dense(10000, activation='relu'))\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 187, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 432, in __call__\n    self.build(input_shapes[0])\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 872, in build\n    constraint=self.kernel_constraint)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 252, in add_weight\n    constraint=constraint)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 399, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 306, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,10000]\n\t [[Node: dense_12/kernel/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@dense_12/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dense_12/kernel, dense_12/random_uniform)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,10000]\n\t [[Node: dense_12/kernel/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@dense_12/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dense_12/kernel, dense_12/random_uniform)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-81a427e4334b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m model.fit(train_in, train_out,\n\u001b[0;32m     35\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m           validation_data=(test_in, test_out))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2652\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2653\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2654\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2655\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[1;31m# hack for list_devices() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,10000]\n\t [[Node: dense_12/kernel/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@dense_12/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dense_12/kernel, dense_12/random_uniform)]]\n\nCaused by op 'dense_12/kernel/Assign', defined at:\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-108-81a427e4334b>\", line 20, in <module>\n    model.add(Dense(10000, activation='relu'))\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 187, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 432, in __call__\n    self.build(input_shapes[0])\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 872, in build\n    constraint=self.kernel_constraint)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 252, in add_weight\n    constraint=constraint)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 399, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 306, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rafael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,10000]\n\t [[Node: dense_12/kernel/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@dense_12/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dense_12/kernel, dense_12/random_uniform)]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adagrad, Adam\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "limit = 20000\n",
    "# limit = len(inputs[])\n",
    "train_size = int(limit * 0.9)\n",
    "test_size = limit - train_size\n",
    "train_in = inputs[ : train_size]\n",
    "train_out = outputs[: train_size]\n",
    "test_in = inputs[train_size : limit]\n",
    "test_out = outputs[train_size : limit]\n",
    "\n",
    "model.add(Dense(10000, activation='relu', input_dim=len(inputs[0])))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10000, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.05)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "#               optimizer='adadelta',\n",
    "              optimizer=sgd,\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_in, train_out,\n",
    "          epochs=20,\n",
    "          validation_data=(test_in, test_out))\n",
    "\n",
    "\n",
    "result = model.evaluate(test_in, test_out)\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, max_features=11, min_samples_split=2, max_depth=None, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(5, weights='distance', n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "# from sklearn import svm\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(data['train']['in'], data['train']['out'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=2000, learning_rate=1.0, max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = BaggingClassifier(LogisticRegression(),\n",
    "                             max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf = VotingClassifier(estimators=[('for', clf_forest), ('naive', clf_naive), ('log', clf_log)], voting='soft', weights=[1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    def fit(inputs, outputs):\n",
    "        pass\n",
    "    \n",
    "    def _pred(self, input):\n",
    "        rad = []\n",
    "        dire = []\n",
    "        \n",
    "        for hero_id, val in enumerate(input):\n",
    "            if val == 1: rad.append(hero_id)\n",
    "            elif val == -1: dire.append(hero_id)\n",
    "          \n",
    "#         sinergy_rad = np.average([wr_pairs[hero, hero] for hero in rad])\n",
    "#         sinergy_dire = np.average([wr_pairs[hero, hero] for hero in dire])\n",
    "        \n",
    "        sinergy_rad = np.average([wr_pairs[hero1, hero2] for hero2 in rad for hero1 in rad])\n",
    "        sinergy_dire = np.average([wr_pairs[hero1, hero2] for hero2 in dire for hero1 in dire])\n",
    "        \n",
    "        #print(sinergy_dire)\n",
    "        \n",
    "        return 1 if sinergy_rad > sinergy_dire else 0\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return [self._pred(input) for input in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Baseline()\n",
    "\n",
    "pred = x.predict(inputs[train_size:])\n",
    "correct = outputs[train_size:]\n",
    "\n",
    "right = 0\n",
    "\n",
    "for p,c in zip(pred,correct):\n",
    "    if p == c:\n",
    "        right += 1\n",
    "\n",
    "print(right / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_size = int(len(inputs) * 0.9)\n",
    "test_size = len(inputs) - train_size\n",
    "\n",
    "clf = clf.fit(inputs[:train_size], outputs[:train_size])\n",
    "preds = clf.predict(inputs[train_size:])\n",
    "correct = outputs[train_size:]\n",
    "\n",
    "\n",
    "print(accuracy_score(correct, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "heroes = pd.read_csv('dota-2-match-prediction-master/data/MatchOverviewTraining.csv').values\n",
    "df_extra = pd.read_csv('dota-2-match-prediction-master/data/MatchDetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(heroes, df_extra):\n",
    "    matches = np.zeros((10000, 121), dtype=int)\n",
    "    out = np.zeros((10000), dtype=int)\n",
    "    \n",
    "    for (i, line) in enumerate(heroes):\n",
    "        out[i] = 1 if line[-1] else 0\n",
    "        match_id = line[0]\n",
    "        heroes = line[1:]\n",
    "        rad = heroes[:5]\n",
    "        dire = heroes[5:-1]\n",
    "        \n",
    "        for hero in rad:\n",
    "            matches[i][hero] = 1\n",
    "            \n",
    "        for hero in dire:\n",
    "            matches[i][hero] = -1\n",
    "            \n",
    "        \n",
    "#         counter = 121\n",
    "        \n",
    "#         exp_gold = df_extra.loc[df_extra['match_id'] == match_id].values\n",
    "#         for (j, data) in enumerate(exp_gold):\n",
    "#             matches[i][counter + 2*j] = data[4]\n",
    "#             matches[i][counter + 2*j + 1] = data[14]\n",
    "            \n",
    "    return matches, out\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = gen_data(heroes, df_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adagrad, Adam\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "train_size = int(len(input) * 0.9)\n",
    "test_size = len(input) - train_size\n",
    "train_in = input[ : train_size]\n",
    "train_out = output[: train_size]\n",
    "test_in = input[train_size : ]\n",
    "test_out = output[train_size : ]\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=len(input[0])))\n",
    "#model.add(Dropout(0.1))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#sgd = SGD(lr=0.05)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              optimizer='adadelta',\n",
    "              #optimizer=sgd,\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(train_in, train_out,\n",
    "          epochs=200,\n",
    "          validation_data=(test_in, test_out))\n",
    "\n",
    "result = model.evaluate(test_in, test_out)\n",
    "\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input[0])\n",
    "clf.predict_proba([input[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "factor = 0.9\n",
    "train_size = int(len(inputs) * factor)\n",
    "test_size = len(inputs) - train_size\n",
    "train_in, train_out, test_in, test_out = inputs[:train_size], outputs[:train_size], inputs[train_size:], outputs[train_size:]\n",
    "\n",
    "\n",
    "clf = clf.fit(train_in, train_out)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "preds = clf.predict(test_in)\n",
    "correct = test_out\n",
    "\n",
    "print(accuracy_score(correct, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = inputs[0]\n",
    "print_heroes(match)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match[114] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match[22] = 0\n",
    "print_heroes(match)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0\n",
    "best_chance = 0\n",
    "\n",
    "for i in range(120):\n",
    "    if match[i] != 0: continue\n",
    "    match[i] = 1\n",
    "    chance_win = clf.predict_proba([match])[0][1]\n",
    "    \n",
    "    if chance_win > best_chance:\n",
    "        best_chance = chance_win\n",
    "        best = i\n",
    "        \n",
    "    print('chance: %f hero: %s' % (chance_win, idToHero.get(i, \"N/A\")))\n",
    "    match[i] = 0\n",
    "\n",
    "print('BEST:', idToHero[best])\n",
    "match[0] = 0\n",
    "print_heroes(match)\n",
    "\n",
    "# print(len([v for v in match if v != 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "# tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "#                      'C': [1, 10, 100, 1000]},\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [10, 50, 100], 'max_features': [11, 40, 121]}]\n",
    "\n",
    "scores = ['precision']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % 'precision')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
